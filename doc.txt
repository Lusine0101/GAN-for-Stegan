     Բովանդակություն
     Ներածություն	3
     Գլուխ 1.	Գրականության վերլուծական ակնարկ	4
     1.1	Մեքենայական ուսուցում	4
     1.1.1	Վերահսկվող ուսուցում	4
     1.1.2	Չվերահսկվող ուսուցում	5
     1.1.3	Որոշ նշանակումներ	5
     1.2	Ուսուցման տարրեր	6
     1.2.1	Արժեքի ֆունկցիա	6
     1.2.2	Նվազող գրադիենտ	7
     1.2.3	Ուսուցման գործակից	9
     1.2.4	Մուտքային տվյալի հատկության մասշտաբավորում	9
     1.3	Պոլինոմալ ռեգրեսիա	10
     1.4	Դասակարգում	11
     1.4.1	Լոգիստիկ հիպոթեզի արժեքի ֆունկցիան	12
     1.5	Նեյրոնային ցանցեր	14
     1.6	Խնդրի դրվածքը	16
     Գլուխ 2.
	Գեներատիվ մրցակցող ցանցերի կիրառումը նկարի տեսքով թաքնագրության կրիչ ստեղծելու համար	17
     1.1	Մրցակցող ցանցեր	17
     1.1.1	Մինիմաքս խաղ	17
     1.1.2	Գեներատիվ մրցակցող ցանցեր	18
     1.2	Խորը փաթույթային գեներատիվ մրցակցող ցանցեր	19
     1.3	Թաքնավերլուծություն մեքենայական ուսուցմամբ	20
     1.4	Թաքնագրության կրիչի գներացիա	22
     1.5	Մոդելների մանրամասն նկարագրություն	24
     1.5.1	Տարբերակիչ	24
     1.5.2	Թաքնավերլուծիչ	25
     1.5.3	Գեներատոր	25
     1.5.4	Մրցակցող մոդելներ	26
     1.6	Ուսուցման տվյալներ	26
     Գլուխ 3.	Բնապահպանություն	28
     1.1	Էկոլոգիական փորձաքննության նպատակները և խնդիրները	28
     Գլուխ 4.	Կենսագործունեության անվտանգություն	28
     1.1	Կլաստերներից առաջացած աղմուկի ազդեցությունը մարդու վրա	28
     Գրականություն	28

Ներածություն
          Այս թեմայի շուրջ 2016թ․-ին կատարվել է հետազոտություն[5],որի ընթացքում փորձել են գեներացնել մարդկանց դեմքեր։ Մոդելը հաջողությամբ մոլորեցրել է թաքնագրային վերլուծիչին,սակայն որոշ դեպքերում մարդու աչքը գեներացված նկարները հեշտությամբ կարող էր տարբերել իրականից,քանզի մոդելին՝ ուսուցման ժամանակ,տրամադրվել էին տարբեր սեռի մարդկանց դեմքեր,սակայն չէին հաշվի առել այդ հանգամանքը։
     Ուսուցմանը մասնակցելու են միանգամից 3 մոդել։ Դրանք են՝
1.	Գեներացնող մոդել (Գեներատոր - Generator) - G
2.	Տարբերակող մոդել (Տարբերակիչ - Discriminator) - D
3.	Թաքնավերլուծող մոդել (Թաքնավերլուծիչ - Steganalyser) - S
     Առաջին մոդելը՝ գեներատորը,պատասխանատու է նկարներ գեներացնելու համար,այն պետք է այնպիսի նկարներ գեներացնի,որ հնարավոր չլինի տարբերել իրական նկարներից։ Այս խնդրի լուծման համար օգտագործվելու է երկրորդ մոդելը՝ տարբերակիչը,որի խնդիրն է լինելու տարբերել իրական նկարը կեղծից (կեղծ են բոլոր այն նկարները որոնք ստեղծել է G գեներատորը)։ Այս ամենից հետո գործի է անցնում 3-րդ մոդելը՝ վերլուծիչը,որի խնդիրն է պարզել արդյո՞ք տրված նկարում առկա է թաքնագրված ինֆորմացիա,թե՞ ոչ։ D վերլուծիչին ուսուցման ընթացքում տրամադրվելու են գեներատորի նկարները,որոնք արդեն պարունակում են թաքնագրված ինֆորմացիա,ինչպես նաև սովորական նկարներ,որոնք չեն պարունակում ոչ մի թաքնագրված ինֆորմացիա։
     Այսպիսով D տարբերակիչն ու S վերլուծիչը բարելավելու են իրենց արդյունքը՝ հիմնվելով G գեներատորի տրամադրած և սովորական նկարների վրա,իսկ G-ն բարելավելու է իր արդյունքը՝ հիմնվելով D-ի և S-ի արդյունքի վրա։ Հենց այստեղ էլ առաջ է գալիս մրցակցող ցանցերի գաղափարը,քանզի ստացվում է,որ ցանցերը մրցում են միմյանց հետ,թե ում արդյունքն ավելի լավը կլինի։
     Վերջերս մշակված մրցակցող ցանցերը[3] հզոր գեներացնող մոդելներ են,որոնց հիմնական գաղափարը գեներատորի և տարբերակիչի ուսուցումն է մինիմաքս խաղի[6] միջոցով: G մոդելը մուտքին ստանում է պատահական՝ այսպես ասած անիմաստ նկար,որի հիման վրա փորձում է ստեղծել հնարավորինս իրականին մոտ պատկեր,իսկ D-ն ձգտում է տարբերակել իրական պատկերները կեղծերից:
     Գոյություն ունեն նմանատիպ ցանցերի տարբեր ձևափոխություններ՝
•	Խորը փաթույթային ստեղծարար մրցակցող ցանցեր[4]
-	այս մոդելը ստեղծարար մրցակցող ցանցի (GAN) փոփոխություն է,որը մասնագիտացված է պատկերների առաջացման ուղղությամբ
•	Պայմանական մրցակցող ցանցեր[7]
-	թույլ է տալիս ստեղծել որևէ դասի օբյեկտներ
•	Պատկերների առաջացում՝ հիմնված տեքստային նկարագրության վրա[8]:
     Թաքնագրվող գաղտնի ինֆորմացիան,ինչպես նաև կրիչը,կարող է ներկայացված լինել տարբեր տեսքով՝ նկարի,տեքստի,տեսահոլովակի,ձայնագրության և այլն։ Այս ուսումնասիրության մեջ կատարվելու է տեքստի թաքնագրում նկարում և օգտագործվելու է DCGAN տեսակը։

Գլուխ 1.	Գրականության վերլուծական ակնարկ
1.1	Մեքենայական ուսուցում
     Նախքան անցնելը բուն թեմային,ծանոթանանք մեքենայական ուսուցման (Machine Learning[9],կրճատ՝ ML) հետ։ Արթուր Սամուելն այն նկարագրում է այսպես «մեքենայական ուսուցումը մի տեխնոլոգիա է,որը համակարգիչներին հնարավորություն է տալիս սովորելու,առանց բացահայտ ծրագրավորված լինելու»: Սա,իհարկե,ոչ պաշտոնական ձևակերպում է,սակայն լավ պատկերացում է տալիս։
     Մեքենայական ուսուցման խնդիրներից են․
•	Վերահսկվող ուսուցում (Supervised learning)
•	Չվերահսկվող ուսուցում (Unsupervised learning)
	Սրա մասնավոր դեպք է խորհրդատու համակարգը (Recommender system)
•	Ուսուցում ամրապնդմամբ (Reinforcement learning)
     Վերահսկվող ուսուցման դեպքում մեքենային տրվում է մուտքային տվյալների հավաքածու և այդ տվյալներին համապատասխան ելքային արժեքները։ Այսպիսով այս ուսուցման դեպքում մեքենային հայտնի են ամեն մի մուտքային ինֆորմացիային համապատասխանող ելքային արժեքը կամ արժեքները։
     
1.1.1	Վերահսկվող ուսուցում
     Վերահսկվող ուսուցման (Supervised Learning) խնդիրները դասակարգվում են հետևյալ 2 տիպերի․ 
•	Ռեգրսիայի խնդիրներ (Regression problems)
•	Դասակարգման խնդիրներ (Classification problems)
     Ռեգրեսիայի խնդրներում փորձում ենք կանխատեսել անընդհատ ֆունկցիայի արժեքներ,ինչը նշանակում է,որ մենք փորձում ենք մուտքային փոփոխականները համապատասխանեցնել ինչ-որ անընդհատ ֆունկցիայի ելքային արժեքներին։ Դասակարգման հարցում մենք փոխարենը փորձում ենք կանխատեսել ընդհատ ելքային արժեքներ: Այլ կերպ ասած,մենք փորձում ենք մուտքային փոփոխականները համապատասխանեցնել դիսկրետ կատեգորիաների:
1.1.2	Չվերահսկվող ուսուցում
     Չվերահսկվող ուսուցումը (Unsupervised Learning) հնարավորություն է տալիս լուծել այնպիսի խնդիրներ,որոնց ելքային արժեքների մասին կա՛մ քիչ ինֆորմացիա ունենք,կա՛մ ընդհանրապես չգիտենք,թե ինչ տեսքի պետք է լինեն: Մենք կարող ենք ստանալ մի այնպիսի ելքային տվյալի կառուցվածք,որի վրա մուտքային տվյալի ազդեցությունն անգամ չգիտենք։ Այդ կառուցվածքը հնարավոր է ստանալ տվյալները համախմբելու արդյունքում՝ հիմնված մուտքային տվյալի փոփոխականների միջև կապերի վրա։ 
     Չվերահսկվող ուսուցման ժամանակ կանխատեսման արդյունքների վրա հիմնված հետադարձ կապ չկա: Այսինքն մոդելը չի փոփոխում իր պարամետրերը՝ հիմնվելով կանխատեսման արդյունքների վրա։
1.1.3	Որոշ նշանակումներ
     Կատարենք մի քանի նշանակումներ,որոնք կոգտագործվեն հետագայում:
     Դիցուք ունենք հետևյալ տվյալները՝
     X1
     …
     Xn
     Y
     Input(1)1
     …
     Input(1)n
     Output(1)
     …
     …
     …
     …
     Input(m)1
     …
     Input(m)n
     Output(m)
      
     X1,X2,… Xn-ը մուտքային պարամետրերի նշանակումներն են,Y-ը՝ ելքային պարամետրի նշանակումը։ Input(i)1,Input(i)2,… Input(i)n-ը մուտքային պարամետրերի արժեքներն են (տվյալի հատկություններ),իսկ Output(i)-ն՝ ելքային պարամետրի արժեքն է,որտեղ՝ i
=1,2,…,m։ Հարմարավետության համար Input(i)1,Input(i)2,… Input(i)n-ը նշանակենք x(i)-ով,իսկ Output(i)-ն՝ y(i)-ով։ Պարզ է,որ՝ n-ը մուտքային պարամետրերի քանակն է։
     (x(i),y(i)) զույգն անվանում ենք ուսուցման օրինակ (training example),իսկ դրանց ցուցակը՝ ուսուցման տվյալներ (training set): Այսինքն m-ը՝ ուսուցման տվյալների քանակն է։
     Այժմ կարող ենք տալ վերահսկվող ուսուցման ավելի ֆորմալ ձևակերպում՝ «Վերահսկվող ուսուցման նպատակն է՝ տրված ուսուցման տվյալների հիման վրա ձևավորել մի այնպիսի h
∶ X 
→ Y ֆունկցիա,որ h(x)-ի ելքային արժեքը բավարար մոտ լինի համապատասխան y-ի արժեքին»։ հ ֆունկցիան անվանում են «հիպոթեզ»։
     Ինչքան h(x)-ի արժեքը մոտ լինի համապատասխան y-ի արժեքին,այնքան ավելի ճիշտ արդյունքներ կտա մեր մեքենայական ուսուցման մոդելը։
     Բնականաբար h(x)-ը ունի գործակիցներ,նշանակենք այդ գործակիցները 
θ0,θ1,… θn - ով,այս պատճառով h(x) - ը որոշ դեպքերում կնշանակենք hθ(x) ։
     Հասկանալի է,որ մեր խնդիրը հենց այդ θ - ների արժեքները գտնելու մեջ է կայանում,քանզի հետագայում՝ երբ արդեն մեր մոդելը բավարար չափով ուսուցանված կլինի,և ունակ կլինի գուշակել ճիշտ արժեքներ,նրան տրվելու են X1,X2,… Xn արժեքները և քանզի այն ունի արդեն հաշվարկած θ0,θ1,… θn արժեքները,ընդամենը պետք է հաշվի hθ(x) - ի արժեքը։
1.2	Ուսուցման տարրեր
1.2.1	Արժեքի ֆունկցիա
     h(x)-ի արժեքների ճշտությունը կարելի է գնահատել արժեքի ֆունկցիայի (Cost Function) միջոցով։ Այն իրենից ներկայացնում է h(x)-ի բոլոր ելքային արժեքների և իրական y-ների արժեքների միջինացված տարբերություն:
     Բանաձևը ներկայացված է ստորև․
     J\left(\theta_0,\ \theta_1,\ldots\theta_n\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h\left(x_i
\right)-\ y_i\right)^2
     Ավելի պարզ այն կարող ենք գրել հետևյալ կերպ՝ \frac{1}{2}\bar{x},որտեղ \bar{x}-ը \left(h_
\theta\left(x_i\right)-y_i
\right)-ի քառակուսային միջինն է,այսինքն՝ գուշակված և իրական արժեքի տարբերությունը։ 
     Այս ֆունկցիան նաև կոչվում է քառակուսային սխալի ֆունկցիա (Squared error function): Քառակուսային միջինը բաժանվել է 2-ի՝ հետագա հաշվարկների հարմարավետության համար,քանի որ դրա միջոցով h\thetaxi-yi2-ի ածանցումից ստացված 2 բազմապատիկը կվերանա։
     Ստացվեց,որ մեր խնդիրը կայանում է J(θ0,θ1,… θn) - ը մինիմիզացնելու մեջ,որն ավելի ֆորմալ կարող ենք ներկայացնել հետևյալ կերպ՝
     \begin{matrix}minimize\\\theta_0,\theta_1,\ldots\theta_n\\\end{matrix}J\left(\theta_0,
\theta_1,\ldots\theta_n\right)
1.2.2	Նվազող գրադիենտ
     Այսիպսով արդեն պարզաբանվեց,թե ինչ է հիպոթեզ ֆունկցիան և թե ինչպես կարելի է չափել նրա ճշտությունը։ Այժմ անհրաժեշտ է որոշել հիպոթեզի պարամետրերը։
     Դիտարկենք հիպոթեզ ֆունկցիայի պարզեցված օրինակ,որն ունի ընդամենը 2 պարամետր՝ θ0 և θ1 ։ Պատկերենք այդպիսի հիպոթեզի արժեքի ֆունկցիայի օրինակ (Նկ․ 1)։ 
      
      Նկ․ 1  2 պարամետրով հիպոթեզի ֆունկցիայի օրինակ
     Այստեղ պետք է հստակ պատկերացնել,որ մենք չենք գծում հիպոթեզի գրաֆիկը,այլ փոխարենը գծում ենք նրա արժեքի ֆունկցիայի գրաֆիկը,որը ցույց է տալիս,թե  
θ0–ի և θ1–ի արժեքների համար ինչքանով է հիպոթեզը շեղված սպասվելիք արժեքներից։ Հասկանալի է,որ պետք է գտնել տվյալ գրաֆիկի վրայի ամենացածր կետը,որի θ0 և θ1 արժեքներն էլ հենց կլինեն մեր հիպոթեզի որոնելի պարամետրերի արժեքները (վերևի նկարում կարմիր սլաքներով նշված են տվյալ գրաֆիկի մինիմումները)։ Քանզի արժեքի ֆունկցիան հիմնականում իրենից ներկայացնում է բարդ մաթեմատիկական բանաձև,այն դժվար է գծել,կամ գտնել,թե θ-ի որ արժեքների դեպքում է այն ընդունում մինիմալ արժեք։ Հենց այս խնդիրը լուծելու համար օգտագործվում է նվազող գրադիենտը (Gradient Descent)։
     Նշվածն իրականացնելու համար կօգտագործենք արժեքի ֆունկցիայի ածանցյալը: Ածանցյալը ցույց է տալիս տվյալ կետում շոշափողի ուղղությունը,որի օգնությամբ ամեն քայլին շարժվում ենք այն ուղղությամբ,որն ամենաշատն է նվազեցնում արժեքի ֆունկցիան։ 
     Յուրաքանչյուր քայլի չափը որոշվում է α պարամետրի միջոցով,որը կոչվում է ուսուցման գործակից (learning rate)։ Փոքր α-ն համապատասխանում է փոքր քայլի,իսկ մեծը՝ մեծ քայլի: Քայլի ուղղությունը,որոշվում է J(θ0,θ1)-ի մասնակի ածանցյալով: Կախված այն բանից,թե որտեղից ենք սկսում դիտարկել գրաֆիկը,հնարավոր է տարբեր մինիմումների հասնել:
     Ընդհանուր դեպքի համար նվազող գրադիենտի ալգորիթմը կլինի․ կրկնել հեևյալը մինչև զուգամիտում՝  
\theta_j≔θj-\alpha\frac{\delta}{\delta\theta_j}Jθ0,θ1,…θn որտեղ`  j 
= 0,1,… n  ներկայացնում է հատկության հերթական համարը: Այն անվանում են նաև թարմացման կանոն (update rule): Մեր օրինակի համար` n 
= 1:
     Յուրաքանչյուր իտերացիային պետք է միաժամանակ թարմացնել բոլոր θ0,θ1,… θn պարամետրերը: 
     Պետք է հաշվի առնել,որ կարևոր է α-ի ճիշտ ընտրությունը,քանզի դրանով է պայմանավորված ալգորիթմի զուգամիտման ժամանակը: Եթե ալգորիթմը չի զուգամիտում կամ շատ ժամանակ է պահանջում մինիմումին հասնելու համար ապա α քայլաչափը սխալ է ընտրված։
     Այստեղ կարող է հարց առաջանալ,թե արդյո՞ք հնարավոր է հասնել մինիմումի՝ α-ի անփոփոխ արժեքի դեպքում։ Պատասխանը պարզ է դառնում,երբ հաշվի ենք առնում այն հանգամանքը,որ,քանզի ամեն քայլ անելուց մենք ավելի ենք իջնում արժեքի ֆունկցիայի մակերևույթով ներքև,հետևաբար ամեն քայլի հետ մեկտեղ ածանցյալի արժեքը նվազում է։ Իսկ դա նշանակում է,որ անգամ,եթե α-ն հաստատուն պահենք,այնուամենայնիվ 
\alpha\frac{\delta}{\delta
\theta_j}Jθ0,θ1,…\thetan արտադրյալը ամեն քայլին կնվազի և հասնելով որևէ մինիմումի այն կհավասարվի 0-ի (իրականում 0-ի չի հավասարվում,այլ մոտենում է ինչ-որ շատ փոքր թվի,որը մեր խնդրի համար համարվում է բավարար) և հետագա քայլերը ոչ մի կերպով չեն ազդի θ- ների արժեքների վրա։
     Հեշտությամբ կարելի է համոզվել,որ,եթե մեր հիպոթեզն ունի գծային տեսք՝
     h_\theta\left(x\right)=\theta_0+\theta_1X_1+\theta_2X_2+\cdots+\theta_nX_n
ապա թարմացման կանոնի մեջ J(θ)-ի արժեքը տեղադրելուց հետո թարմացման կանոնի տեսքը կլինի՝ 
     \theta_j≔θj-α1mi=1mhθxi-yi⋅xji
որտեղ՝ j≔0…n:
     Այստեղ և հետագայում կընդունենք,որ x_0^{\left(i\right)}
=1,բոլոր i-երի համար: Սա արվում է բանաձևերը հարմար ներկայացնելու համար։
1.2.3	Ուսուցման գործակից
     Նվազող գրադիենտն իրականացնելուց հետո անհրաժեշտ է հետևել ալգորիթմի աշխատանքին (մոդելի ուսուցման պրոցեսին) և հասկանալ արդյո՞ք այն ճիշտ է աշխատում։
     Պատկերացում կազմելու համար,թե ինչքան լավ է սովորում մոդելը,անհրաժեշտ է գծել արժեքի ֆունկցիայի՝ J(θ)-ի,կախումը իտերացիաների քանակից։ Պարզ է,որ,եթե ամեն ինչ ճիշտ է աշխատում,ապա ամեն իտերացիայից հետո J(θ)-ի արժեքը պետք է նվազի՝ ձգտելով 0-ի։ Հետևաբար,եթե գրաֆիկը աճում է,ապա ինչ որ բան այն չէ։ Հիմնականում դրա պատճառը α-ի մեծ արժեքն է լինում․ անհաժեշտ է նվազեցնել α-ի արժեքը։
     Հարկ է նշել՝ ապացուցված է,որ,եթե ուսուցման գործակից (Learning Rate) α-ն բավարար չափով փոքր է ընտրված,ապա J(θ)-ն նվազում է ամեն իտերացիային։ Սակայն,եթե այն շատ փոքր է ընտրված,ապա J(θ)-ն կարող է շատ դանդաղ նվազել։ 
     Կարելի է համարել որ մոդելը բավարար չափով ուսուցանվել է այն պահին,երբ J(θ)-ի փոփոխությունն ինչ-որ իտերացիայից հետո ավելի փոքր է որևէ E արժեքից։ E-ն կամայապես ընտրված փոքր թիվ է,օրինակ՝ 10-3։ Գործնականում դժվար է ընտրել  
E-ի օպտիմալ արժեք։
1.2.4	 Մուտքային տվյալի հատկության մասշտաբավորում
     Մենք կարող ենք արագացնել նվազող գրադիենտի աշխատանքը` բերելով բոլոր մուտքային պարամետրերը մոտավորապես նույն տիրույթի թվերի: Դա կապված է այն բանի հետ,որ որ θ-ն ավելի արագ է հասնում մինիմումին փոքր միջակայքերում և ավելի դանդաղ՝ մեծ միջակայքերում,հետևաբար այն տատանվելով է այն տատանվելով է ձգտում մինիմումին,երբ փոփոխականները շատ անհավասար են:
     Դա կանխելու համար կարող ենք այնպես փոփոխել հատկությունները (մուտքային պարամետրերը),որ նրանք ընկնեն մոտավորապես միևնույն թվային տիրույթ։ Իդեալական դեպքում՝ -1
<x_i<1 կամ՝\ -0.5<x_i<0.5։
     Սրանք պարտադիր պահանջներ չեն,մենք ընդամենը փորձում ենք կրճատել հաշվարկների ժամանակը: Նպատակն է՝ բերել բոլոր մուտքային փոփոխականները միևնույն տիրույթի:
     Հարկ է նշել նաև,որ,եթե չկատարվի հատկությունների մասշտաբավորում,ապա որոշ դեպքերում հնարավոր է,որ ալգորիթմը երբեք չզուգամիտի։
     Հատկության մասշտաբավորումն[10] (Feature Scaling) ու միջինով նորմալացումը (mean normalization) այն երկու մեթոդներն են,որոնք կօգնեն լուծել այդ խնդիրը: Առաջինը ենթադրում է մուտքային տվյալների բաժանում նրանց մեծագույն և փոքրագույն արժեքների տարբերության վրա։ Միջինով նորմալացման դեպքում պետք է մուտքային փոփոխականից հանել մուտքային տվյալների միջին արժեքը,ապա նոր բաժանել մեծագույն և փոքրագույն արժեքների տարբերության վրա։ Ստացվեց,որ այս երկու մեթոդների իրականացման համար անհրաժեշտ է փոփոխել մուտքային պարամետրերը՝ համապատասխան ներքևի բանաձևի․
     x_i≔xi-μisi
որտեղ s_i-ն i-րդ հատկության մեծագույն և փոքրագույն արժեքների տարբերությունն է,իսկ 
\mu_i-ն՝ այդ հատկության բոլոր արժեքների միջինը։ Նշենք,որ s_i-ին կարող ենք ընդունել հավասար մրջին քառակուսային շեղմանը,և այդ դեպքում ստացված արժեքները կտարբերվեն նախորդ տարբերակով ստացված արժեքներից։
1.3	Պոլինոմալ ռեգրեսիա
     Բնականաբար հիպոթեզ ֆունկցիան կարող է լինել կամայական տեսակի։ Նրա տեսքը պարզելու համար անհրաժեշտ է կատարել տվյալների ուսումնասիրություն։ Եթե ուսումնասիրությունից հետո պարզվում է,որ հիպոթեզը չպետք է լինի գծային,ապա կարևոր է իմանալ,որ հնարավոր է ձևափոխել այն քառակուսայինի,խորանարդայինի կամ այլ տեսքի կորի։
     Օրինակ,եթե մեր հիպոթեզ ֆունկցիան ունի հետևյալ տեսքի է՝
     h_\theta\left(x\right)=\theta_0+\theta_1x_1
ապա կարելի է ստեղծել նոր հատկություններ՝ հիմնված x_1-
-ի վրա այնպես,որ ստանանք քառակուսային ֆունկցիա՝ h_\theta\left(x\right)=\theta_0+
\theta_1x_1+\theta_2x_1^2 կամ՝ խորանարդային ֆունկցիա՝ h_\theta\left(x\right)=\theta_0
+\theta_1x_1+\theta_2x_1^2+
\theta_3x_1^3։ Այս օրինակներում ստեղծեցինք նոր` x_2 և x_3,հատկություններ,որտեղ x_2
=x_1^2,իսկ x_3
=x_1^3: Այն քառակուսի արմատի տեքի դարձնելու համար,կարելի է կատարել հետևյալ ձևափոխությունը՝ h_
\theta\left(x\right)=\theta_0+\theta_1x_1+\theta_2\sqrt{x_1}։
     Նշված ոչ գծային հիպոթեզները պոլինոմալ ռեգրեսսիայի (Polynomial Regression) օրինակներ են։
     Հարկ է հիշել,որ նշված կերպով հատկություններ ավելացնելիս շատ կարևոր է կատարել հատկությունների մաշտաբավորում,քանի որ հատկությունների տիրույթներն իրարից խիստ տարբերվելու են։
1.4	Դասակարգում
     Որպես դասակարգման խնդիր լուծելու մեթոդ կարելի է օգտագրոծել գծային ռեգրեսիան և 0.5-ից մեծ գուշակված արժեքներն ընդունել որպես 1,իսկ դրանից փոքրերը՝ 0։ Սակայն այս մեթոդը լավ չի աշխատում,քանի որ դասակարգուման հիպոթեզն իրականում գծային ֆունկցիա չէ։ Այն ռեգրեսիայի խնդիր է,սկայան այն տարբերությամբ,որ նրա արժեքները վերջավոր քանակի դիսկրետ արժեքներ են։
     Մինչ ավելի բարդ դեպքերի անցնելը,կենտրոնանաք երկուական դասակարգման խնդրի (binary classification problem) վրա,որտեղ y- ը կարող է ընդունել միայն 2 արժեք՝ 0 և 1։ 
     Դասակարգման խնդրի լուծելու համար կարող ենք անտեսել այն հանգամանքը,որ սպասվող ելքը վերջավոր,դիսկրետ արժեքներ են և օգտագօրծենք գծային ռեգրեսիան այս խնդրի լուծման համար։ Սակայն այս դեպքում անգամ անիմաստ են hθ(x)-ի 1-ից մեծ և 0-ից փոքր արժեքները,քանի որ մենք գիտենք,որ y
\in\left\{0,\ 1\right
\}։ Սրան լուծում տալու համար կձևափոխենք hθ(x)-ն այնպես,որ նա բավարարի 0 
≤ hθ(x) 
≤ 1 պայմանը։ Դա անելու համար կարելի է լոգիստիկ ֆունկցիային (Logistic Function) փոխանցել θTx-ը՝
     h_\theta\left(x\right)=g\left(\theta^Tx\right)=\frac{1}{1+e^{-\theta^Tx}}
     Այստեղ g-ն հենց այն լոգիստիկ ֆունկցիան է,որը կամայական իրական թիվ համապատասխանեցնում է (0,1) տիրույթի որևէ թվի,ինչը թույլ է տալիս կամայական տիրույթի ելքային արժեքներ ունեցող ֆունկցիան փոխակերպել դասակարգման խնդրին ավելի հարմար ֆունկցիայի։ Լոգիստիկ ֆունկցիան նաև անվանում են Սիգմոիդ ֆունկցիա (Sigmoid Function)։ 
     Նկ․ 2-ում պատկերված է այդպիսի ֆունկցիայի մի օրինակ։
      
      Նկ․ 2  Սիգմոիդ ֆունկցիայի օրինակ
     Այսպիսով hθ(x)-ը հավանականնությունն է այն բանի,որ ելքային արժեքը հավասար է 1-ի։ 
1.4.1	Լոգիստիկ հիպոթեզի արժեքի ֆունկցիան
     Ընդհանուր դեպքում արժեքի ֆունկցիան ունի հետևյալ տեսքը՝
     J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}Cost\left(h_\theta\left(x_i\right),\ y_i\right)
     որտեղ Cost-ը այն ֆունկցիան է,որը հաշվում է արժեքը i-րդ ուսուցման օրինակի համար։ 
     Արդեն նշվել է,որ գծային հիպոթեզի դեպքում՝ Cost\left(h\left(x_i\right),\ y_i\right)=\left(h
\left(x_i\right),\ y_i
\right)^2,սակայն լոգիստիկ ֆունկցիայի համար չի կարելի օգտագործել նույն բանաձը,քանի որ այդ կերպ ստացված J ֆունկցիան ալիքային տեսքի է և հետևաբար ունի բազմաթիվ լոկալ մինիմումներ,որոնք բարդացնում են գլոբալ մինիմումը գտնելը։ Այլ կերպ ասած J-ի գրաֆիկը ուռուցիկ չի լինի։
     Ասվածն ավելի լավ պատկերացնելու համար Նկ․ 7-ում բերված են ուռուցիկ և ոչ ուռուցիկ ֆունկցիաների գրաֆիկների oրինակներ։
      
      Նկ․ 7  Ուռուցիկ և ոչ ուռուցիկ ֆունկցիաների գրաֆիկների որինակ
     Փոխարենը կարելի է օգտագործել հետևյալ ֆունկցիան՝
     Costhθx,y=-loghθx          երբ՝  y=1Costhθx,y=-log1-hθx  երբ՝  y=0
     Այստեղից երևում է,որ,եթե արժեքի ֆունկցիան գրենք այս ձևով,ապա համոզված կարող ենք ասել,որ J-ն ունի ուռուցիկ տեսք լոգիստիկ ռեգրեսիիայի համար։ Ինչը շատ կարևոր է ավելի արագ ուսուցանվող և ճիշտ արդյունքներ գուշակող մոդել ստեղծելու համար։ Ելնելով գրաֆիկից կարող ենք ասել․
•	Երբ y=
0,ապա արժեքի ֆունկցիան կլինի 0,միայն,եթե հիպոթեզի ֆունկցիայի ելքում նույնպես ստացվի 0։ Եթե հիպոթեզը ձգտում է 1-ի,ապա արժեքի ֆունկցիան կձգտի անվերջության։
•	Երբ y=
1,ապա արժեքի ֆունկցիան կլինի 0,միայն,եթե հիպոթեզի ֆունկցիայի ելքում նույնպես ստացվի 1։ Եթե հիպոթեզը ձգտում է 0-ի,ապա արժեքի ֆունկցիան կձգտի անվերջության։
     Ասվածը մաթեմատիկորեն կարող ենք ներկայացնել հետևյալ կերպ՝
     •hθxi=y⇒Costhθxi yi=0    •y=0          hθxi→1⇒Costhθxi yi→∞•y=1          hθxi→0⇒Costhθxi yi→∞
որը կարելի է փոխարինել մեկ արտահայտությամբ հետևյալ կերպ՝
     Cost\left(h_\theta\left(x\right),\ y\right)=-y\log{\left(h_\theta\left(x\right)\right)}-\left(1-y
\right)log{\left(1-h_\theta\left(x\right)\right)}
     Հետևաբար J-ն կունենա հետևյալ տեսքը՝
     J\left(\theta\right)=-\frac{1}{m}\sum_{i=1}^{m}\left[-y^{\left(i\right)}\log{\left(h_\theta
\left(x^{\left(i\right)}\right)\right)}-\left(1-y^{\left(i\right)}\right)log{\left(1-h_\theta
\left(x^{\left(i\right)}\right)\right)}\right]
     Կատարելով մաթեմատիկական ձևափոխություններ կարելի է համոզվել,որ այս դեպքում թարմացման կանոնը կլինի նույնն ինչ գծային ռեգրեսիայի համար՝
     \theta_j≔θj-α1mi=1mhθxi-yi⋅xji
որտեղ j=0,1,…n:
1.5	Նեյրոնային ցանցեր
          Հասկանալու համար,թե ինչ անհրաժեշտություն կա ուսումնասիրել այլ ուսուցման ալգորիթմ՝ նեյրոնային ցանցեր[11] (Neural Networks),պատկերացնենք մի դեպք,երբ դասակարգման խնդիր լուծելիս հատկությունները բավարար չեն ճշգրիտ մոդել ուսուցանելու համար,և անհրաժեշտություն է առաջացել ավելացնել նոր՝ քառակուսային,խորանարդային կամ այլ,հատկություններ։ Այս դեպքում եթե ավելացնենք բոլոր քառակուսային հատկությունները՝
     \begin{matrix}\begin{matrix}x_1^2,x_1x_2,x_1x_3,\ ...x_1x_n\\x_2^2,x_2x_3,...x_2x_n\\.\ .\ .
\\\end{matrix}\\x_{n-1}^2,x_{n-1}x_n\\x_n^2\\\end{matrix}
ապա կստանանք \frac{n\cdot\left(n+1\right)}{2}
+n քանակի հատկություն։ Այսինքն ստացվում է,որ նոր հատկությունների քանակը նախկինից մոտավորապես քառակուսային 
\left(\approx\frac{n^2}{2}
\right) կախում ունի։ Նույն ձևով խորանարդային հատկություններ ավելացնելիս կարելի է համոզվել որ կախումը խորանարդային է։ Սա կարող է բերել գերհամապատասխանեցման (overfitting) խնդրին ինչպես նաև բավականաչափ մեծ հաշվողական ռեսուրսներ կպահանջվեն նման մեծ թվով հատկությունների հետ աշխատելու համար։
          Մեքենայական ուսուցման մեջ օգտագործվող նեյրոնի մոդելը հնարավորինս մոտ է արված մարդու ուղեղի նեյրոնային կառուցվածքին։ Յուրաքանչյուր նեյրոն ստանում է մուտքին որևէ պարամետրեր,կատարում է որոշակի հաշվարկներ,և արդյուների հիման վրա որոշում է թե ինչ ազդանշան ուղարկի հաջորդ նեյրոնին:
      
      Նկ․ 9  Նեյրոնի մոդելի օրինակ
     Նկ․ 9-ում պատկերված է նեյրոնի պարզեցված մոդելը,որն օգտագործվում է մեքենայական ուսուցման մեջ։ x1,x2,x3-ը մուտքային պարամետրերն են,իսկ դեղինով եզրագծվածը նեյրոնի «մարմինն» է։ Այստեղ նույնպես կարող ենք ավելացնել x_0
=1 պարամետրը,որը կոչվում է շեղում (bias)։ Նեյրոնի կատարած հաշվարկների արդյունքը հիպոթեզ ֆունկցիայի արժեքն է,որի բանաձևը լոգիստիկ ռեգրեսիայի բանաձևն է։ Նեյրոնի հիպոթեզի ֆունկցիան այլ կերպ անվանում են նաև սիգմոիդ ակտիվացման ֆունկցիա։ Նեյրոնի ակտիվացիան դա լոկ այն արժեքն է,որը հաշվարկում է այդ նեյրոնը,այլ կերպ ասած նրա ելքում ստացված արժեքն է։ Բնականաբար այդ ֆունկցիան,ինչպես և նախորդ մեր դիտարկած սիգմոիդ ֆունկցիան ունի իր պարամետրերը՝ 
\theta_0,\ \theta_1,\ldots\theta_n (մեր օրինակի դեպքում n 
= 3),որոնց անվանում են կշիռներ (weights)։
     Նեյրոնային ցանցը,ինչպես բխում է անունից,բազմաթիվ նեյրոններից բաղկացած ցանց է,որոնց ելքերն ու մուտքերը կապված են միմյանց հետ։ Նկ․ 10-ում պատկերված է նեյրոնային ցանցի մի պարզ օրինակ։
     Ցանցը բաժանվում է շերտերի (layers)։ Առաջին շերտը անվանում են մուտքային շերտ,քանի որ սա այն շերտն է որտեղ գտնվում են հատկությունները։ Վերջին շերտը անվանում են ելքային շերտ,այն հաշվարկում է հիպոթեզ ֆունկցիայի վերջնական արժեքը։ Առաջին և վերջին շերտերի միջև ընկաց բոլոր մնացած շերտերն անվանում են թաքնված շերտեր։ Վերջիններս թաքնված են,քանի որ ուսուցման ընթացքում նրանց արժեքներին չենք հետևում։ a_i^{
\left(j\right)}-ով կնշանակված է j-րդ շերտի i-րդ նեյրոնի ակտիվացիան»,իսկ \theta^{\left(j
\right)}-ով կնշանակենք կշիռների այն մատրիցը,որը պարունակում է j-ից (j
+1) շերտ անցնելու բոլոր ակտիվացիաների ֆունկցիաների պարամետրերը։ 
      Նկ․ 10  Նեյրոնային ցանցի պարզ օրինակ
     Ընդհանուր դեպքում (j
+1)-րդ շերտի նեյրոնների ակտիվացման ֆունկցիաների տեսքը բերված է ներքևում․
     \begin{matrix}a_1^{\left(j+1\right)}=g(\theta_{10}^{(j)}x_0+\theta_{11}^{(j)}x_1+...+
\theta_{1n}^{(j)}x_n)\\.\ .\ .\\a_{s_{j+1}}^{\left(j+1\right)}=g(\theta_{s_{j+1}0}^{(j)}x_0
+\theta_{s_{j+1}1}^{(j)}x_1+...+\theta_{s_{j+1}n}^{(j)}x_n)\\\end{matrix}
որտեղ n-ը մուտքային պարամետրերի քանակն է,իսկ s_{j+1}-ը՝ (j
+1)-րդ շերտում նեյրոնների քանակը։ g-ֆունկցիան արդեն պարզաբանվել է 1.4 բաժնում։ Նշված ֆունկցիաների օգնությամբ վերջին շերտի արժեքը հաշվելով կարող ենք ստանալ h_
\theta\left(x\right)-ի արժեքը։
1.6	Խնդրի դրվածքը
     Ուսումնասիրելով գրականությանը կարելի է եզրահանգել,որ ավարտական աշխատանքի շրջանակներում դրվում է խնդիր` մշակել գեներատիվ մրցակցային ցանցերի միջոցով նկարի տեսքով թաքնագրության կրիչ (կոնտեյներ) ստեղծող համակարգ։
     Այդ նպատակով անհրաժեշտ է ուսուցանել միաժամանակ 3 մոդել՝ գեներատոր,տարբերակիչ,թաքնավերլուծիչ։ Այս մոդելները մրցակցելով միմյանց հետ փորձելու են լավորակել նախագծվող համակարգի արդյունքը,որից հետո համոզվելով,որ մոդելները բավարար չափով ուսուցանված են,գեներացնող մոդելը կկարողանա ստեղծել իրական մարդկանց դեմքերին մոտ այնպիսի նկար-կրիչներ,որոնք կապահովեն բարձր թաքնակայունություն։
     2016 թվականին կատարված հետազոտությունները լավ արդյունքներ էին տվել,սակայն գեներացված նկարները մոտ չեին իրական նկարներին։ Տվյալ աշխատության մեջ գեներատորին ուսուցանման ժամանակ տրվելու են ինչ-որ հատկանիշներով (սեռ,տարիք,ռասսա) նման մարդկանց նկարներ,ինչը,ենթադրվում է,որ կհանգեցնի իրականին ավելի մոտ նկարների ստեղծմանը։ 
Գլուխ 2.	Գեներատիվ մրցակցող ցանցերի կիրառումը նկարի տեսքով թաքնագրության կրիչ ստեղծելու համար
1.1	Մրցակցող ցանցեր
1.1.1	Մինիմաքս խաղ
     Մինիմաքս-ը որոշումներ ընդունելու կանոն է,որն օգտագործվում է արհեստական բանականությունության,որոշումների տեսության,խաղերի տեսության,ստատիստիկայի և փիլիսոփայության մեջ,հնարավոր կորուստը (վատագույն դեպքում՝ մաքսիմալ կորուստը) քչացնելու համար։ Սկզբում կանոնները նախատեսված էին երկու խաղացողից բաղկացած զրոյական գումարով խաղի համար,որտեղ մի խաղացողի հաղթանակը բացառում է մյուսի հաղթանակը։ Հետագայում այն զարգացել է և օգտագործվում է ավելի բարդ խաղերում,ինչպես նաև անորոշության պայմաններում որոշումների ընդունման մեջ։
     Խաղացողի մաքսիմալ վաստակած միավորը դա այն ամենամեծ թիվն է,որը խաղացողը կարող է հավաքել,առանց իմանալու հակառակորդների քայլերը։ Համապատասխանաբար այդ միավորը այն ամենափոքր թիվն է,որը հակառակորդները կարող են ստիպել խաղացողին հավաքել,երբ գիտեն նրա քայլերը։ Ասվածը մաթեմատիկորեն կարող ենք ներկայացնել հետևյալ կերպ՝
      
     Որտեղ՝
•	i-ն հերթական խաղացողի համարն է
•	-i-ն բոլոր խաղացողներն են՝ բացառությամբ i-րդի
•	a_i-ն i-րդ խաղացողի քայլն է
•	a_{-i}-ն բոլոր խաղացողների քայլերն են՝ բացառությամբ i-րդի
•	v_i i-րդ խաղացողի միավորների հաշվման (արժեքի) ֆունկցիան է
     i-րդ խաղացողի մաքսիմալ միավորի հաշվարկը կատարվում է հաշվի առնելով վատագույն տարբերակը՝ նրա ամեն մի հնարավոր քայլի համար ստուգվում է մնացած խաղացողների հնարավոր բոլոր քայլերը և գտնվում է վատագույն կոմբինացիան,որը խաղացողին կբերի ամենափոքր միավորը։ Հետո պետք է հասկանալ,թե,ինչ քայլ պետք է անի i-րդ խաղացողը,որպեսզի համոզված լինի,որ  այս ամենափոքր միավորը դա նրա ամենամեծ հնարավոր միավորն է։ Այսինքն i-րդ խաղացողն իր հերթական քայլով մաքսիմիզացնում է իր միավորը և մինիմիզացնում է հակառակորդների ազդեցությունը իր միավորի վրա։
     Այն խաղը որում հնարավոր է կիրառել վերը նշված մոտեցումը,անվանում են «Մինիմաքս խաղ»։
1.1.2	Գեներատիվ մրցակցող ցանցեր
          ԳՄՑ-ի միջոցով կարելի է ստանալ հզոր նկարներ գեներատիվ մոդելներ,սակայն ստացված մոդելներն ունակ չեն գեներացնել կամայական տիպի տվյալներ։ Այդ մոդելները գեներացնում են միայն այնպիսի տվյալներ,ինչի վրա որ կատարվել է ուսուցումը։ Այսպիսով,եթե ԳՄՑ-ի ուսուցման ժամանակ տրվել են միայն շան նկարներ,ապա գեներատիվ մոդելը կսովորի գեներացնել միայն շան նկարներ,և ունակ չի լինի գեներացնել բոլորովին այլ տիպի կենդանու նկարներ։ Իսկ եթե մուտքային տվյալները բավականին տարբեր բնույթի լինեն,ապա հնարավոր է որ գեներատորի ուսուցումը անհաջող լինի և այն վերջիվերջո ունակ չլինի գեներացնել որևէ իմաստ արտահայտող տվյալներ։
     Տվյալ աշխատությունում ուսուցման ժամանակ G գեներատորի մուտքին տրվելու է որևէ z բաշխումից աղմուկ՝ p_z(z),որից G-ն ստանալու է նոր նկար,այդ ֆունկցիան նշանակենք՝ G(z;
\ \theta_g): Սահմանենք ևս մեկ ֆունկցիա տարբերակիչի համար՝ D(x;\ 
\theta_d),որի ելքը մի սկալյար մեծություն է,որն արտահայտում է նրա մուտքին տրված 
\ x նկարի իրական լինելու հավանականությունը։ Այսինքն նրա ելքը կլինի 0,եթե մուտքին տրված նկարը գեներացված է,և 1՝ հակառակ դեպքում։ Ստացվում է,որ 
\ D-ն մեզ մոտ երկուական դասակարգիչ է։ Ուսուցման ընթացքում մենք փորձում ենք մեծացնել 
\ D-ի ճշտությունը,նրա մուտքին տալով իրական և գեներատորի գեներացրած նկարներ։ Միևնույն ժամանակ՝ զուգահեռաբար,մենք ուսուցանում ենք G-ն ստիպելով նրան փոքրացնել D-ի ճշտությունը։ Այլ կերպ ասած այս երկու մոդելները մրցակցում են միմյանց հետ և խաղում են հետևյալ մինիմաքս խաղը V(G,D) արժեքի ֆունկցիայով`
      
     ԳՄՑ ուսուցանելիս պետք է միշտ հիշել,որ մենք ուսուցանում ենք գեներատորը և տարբերակիչը զուգահեռաբար։ Այսինքն ուսուցման մեկ տակտի ընթացքում նեյրոնային ցանցի կշիռները թարմացնում է թե՛ գեներացնող և թե՛ տարբերակող մոդելը։ Նկ․ 11-ում պատկերված են ուսուցմանը մասնակցող նեյրոնային ցանցերը և նրանց միջև կապերն ու մուտքերը։
      
      Նկ․ 11 Գեներատիվ մրցակցող ցանցերի աշխատանքի սխեմա
1.2	Խորը փաթույթային գեներատիվ մրցակցող ցանցեր
     Ներկա պահին գոյություն ունեն ԳՄՑ-երի տասնյակ տարբեր տեսակներ։ Խորը փաթույթային գեներատիվ մրցակցող ցանցերը (ԽՓԳՄՑ) դրանցից մեկն է։ ԽՓԳՄՑ-ն նախատեսված է նկարների գեներացիայի համար։ Երկար տարիներ ուշադրության կենտրոնում են եղել վերահսկվող փաթույթային նեյրոնային ցանցերը,մինչդեռ ԽՓԳՄՑ-ն լավ օրինակ է չվերահսկվող փաթույթային նեյրոնային ցանցերի ոչ պակաս արդյունավետության։ Ինչպես երևում է անվանումից,ԽՓԳՄՑ-երի հիմքում ընկած է փաթույթային ցանցերի գաղափարը,որոշակի փոփոխություններով,որոնք առաջ են քաշվել վերջերս։ Դրանք են՝
1.	Նեյրոնային ցանցի բոլոր ոչ-փաթույթային` միավորման (pooling),շերտերը փոխարինել փաթույթայինով,ինչը հնարավորություն կտա ցանցին սովորել մշակել սեփական downsampling-ը։ Այս աշխատությունում այս մոտեցումը օգտագործվել է նաև գեներատորի մոդելավորման համար,ինչը թույլ է տալիս գեներատորին մշակել իր սեփական upsampling-ը։
2.	Վերացնել ամբողջությամբ միացված շերտերը փաթույթային շերտերից առաջ։
3.	Անհրաժեշտ է կիրառել խմբային նորմալիզացում (Batch Normalization),ինչը կկայունացնի ուսուցումը։ Այն կնորմալիզացնի ամեն նեյրոնի մուտքը՝ բերելով միջին արժեքը զրոյի։
4.	Տարբերակիչի բոլոր շերտերի համար օգտագործել բացվածքով ReLU (Leaky ReLU) ֆունկցիան ReLU-ի փոխարեն։
1.3	Թաքնավերլուծություն մեքենայական ուսուցմամբ
     Թաքնագրության ժամանակ կոնտեյներում թաքցվում է գաղտնի տվյալը։ Կոնտեյներների դասին են պատկանում նաև նկարները: Նկարներում՝ նրա պիկսելային ներկայացման մեջ ինֆորմացիայի թաքնագրման ժամանակ կատարվում է փոփոխություն նակրի տենզորի մեջ,որը ունի NxMxC չափողականություն,որտեղ՝
1.	N-ը տողերի քանակն է
2.	M-ը սյուների քանակն է
3.	C-ը գույների խորությունն է կամ նկարի հոսքերի թիվը
     Հաշվի առնելով այն,որ նկարներում հիմնականում օգտագործվում է 8 բիթ կոդավորում,կարելի է հաշվարկել նկարի տենզորի չափը բիթերով՝
S=N\ast M\ast C\ast8
     Նկարի տենզորի մեջ թաքցվող ինֆորմացիայի քանակը համեմատական է նրա չափին՝
     T=Kնորմ*S
     Kնորմ գործակիցը բնութագրում է նկարի կոնտեքստից,որն իր հերթին իրենից ներկայացնում է տենզորում պիկսելների բաշխման ֆունկցիա։ Kնորմ-ը խիստ կախվածություն ունի պիկսելների բաշխումից և որպես հետևյանք երկու նույն չափի նկարների թաքնագրման տարողունակությունը կարող է խիստ տարբերվել։ Kնորմ-ն իրենից կրում է զուտ բնութագրական բնույթ և իհարկե հնարավոր է գերազանցել թույլատրելի նորման,սակայն նմանատիպ մոտեցումը կբերի թաքնագրային համակարգի վատթարացմանը և հետագա անվտանգության նվազեցմանը։
     	Ինչպես արդեն նշվեց Kնորմ-ը հանդիսանում է ֆունկցիա պիսկելների բաշխումից՝ 
Kնորմ=K(p)

     Այստեղ p-ն հանդիսանում է պիկսելների բաշխման ֆունկցիան։ K-ն բավականին դժվար է գնահատել և դժվար է այն ներկայացնել անալիտիկ տեսքով,հաշվի առնելով գոյություն ունեցող նկարների տարատեսակը։ 
     	Մեքենայական ուսուցման հիմնախնդիրներից է մոտարկել ֆունկցիան,ըստ մուտքային տվյալների։ Այս մոտեցումը խոստումնալից է K-ի տեսքի որոնման հարցում,քանի որ կարելի է բավականին ճշգրիտ մոտարկել K ֆունկցիան։ Առաջարկվող համակարգում K ֆունկցիայի մոտարկումը պարամետրիզացվում է նեյրոնային ցանցով։
     Kնորմ,մոտ=Kw(pտվյալ)
     Քանզի խնդիրը կայանում է գեներացնել նկարներ մաքսիմալ Kնորմ-ով,այս աշխատությունում ներկայացվող համակրգում ներդրվում է դասակարգիչ,որի հիմնական նպատակն է հասկանալ արդյոք առկա է նկարում թաքնագրված տվյալ,նույնն է թե արդյոք գերազանցվել է Kնորմ-ը տվյալ կոնտեյների համար։ Քանի որ առաջարկվող մոտեցման մեջ խնդիրը դա մեծ Kնորմ-ով կոնտեյների գեներացումն է,ուսուցման ժամանակ եթե դասակարգիչը հայտանբերում է թաքնագրված տեքստ ապա Kնորմ մոտարկող նեյոնային ցանցը,որը մոտարկումը կատարում է համապատասխան Kնորմ-ի համար նկարի գեներացմամբ ենթարկվում է պարամետերերի թարմացամն ըստ գրադիենտային անկման։ Այսպիսի մոտեցումը թույլ է տալիս մաքսիմալացնել գեներացվող կոնտեյներների Kնորմ-ը և որպես հետևանք արդյունքում ստանալ մեծ տարողունակությամբ կոնտեյներ նկարներ։
     	Թաքնագված տվյալների դասակարգիչը ուսուցանվում է գեներատորի հետ միասին և ենթարվում է թարմացման՝ սխալ դասակարգման ժամանակ։ Այս մոտեցումը թույլ է տալիս ստանալ տվյալ կոտեքստով նկարների համար բարձր ճշգրտության դասակարգիչ։ Այդ դասակարգիչը չի հանդիսանում համապիտանի քանի որ նա կարող է գնահատել միայն ցածր պարամետրիզացիա ունեցող տվյալների բաշխման Kնորմ։ Առաջարկվող մոտեցումը սահմանապակում է տվյալների բաշխումը,օգտագործելով միայն մարդկանց դեմքերի սահմանափակ խումբ։
1.4	Թաքնագրության կրիչի գներացիա
     Տվյալ աշխատությունում ներկայցված համակարգում իրար են միացված 3 մոդել՝ 
•	G գեներատորի ցանցը,որը գեներացնում է իրականին մոտ նկարներ 
•	D տարբերակիչը,որը որոշում է արդյո՞ք նկարը իրական է թե գեներացված
•	S թաքնավերլուծիչը,որը որոշում է արդյո՞ք նկարը պարունակում է թաքնագրված ինֆորմացիա,թե ոչ
     Ընդ որում գեներատորն ու տարբերակիչը իրենցից ներկայացնում են ԽՓԳՄՑ։ Այս համակարգի հիմնական տարբերությունը սովորական ԳՄՑ-ներից այն է,որ ուսուցման ընթացքում գեներատորն իր կշիռները թարմացնում է միաժամանակ հիմնվելով երկու մոդելների՝ տարբերակիչի և թաքնավերլուծիչի,արդյունքների վրա։ Գեներատորը փորձում է մեծացնել D-ի և S-ի սխալանքը,միևնույն ժամանակ վերջիններս փորձում են քչացնել այն։
     Ներքևի հավասարումը վերը նշված օպտիմիզացիայի խնդրի մաթեմատիկական ներկայացումն է,որտեղից երևում է,որ գեներատորը խաղում է մինիմաքս խաղը միաժամանակ D-ի և S-ի հետ՝
      
     Որտեղ՝
•	p_{data}(x)-ը x բաշխումից ստացված իրական նկարներն են,որի հիման վրա պետք է D-ն սովորի տարբերակել իրական նկարները գեներացվա
•	p_{noise}(z)-ը z բաշխումից ստացված աղմուկն է,որի հիման վրա G-ն պետք է գեներացնի նոր նկարներ
•	Stego(x)-ը մի ֆունկցիա է,որը x նկարի մեջ կատարում է թաքնագրում և վերադարձնում է արդեն թաքնագրված ինֆորմացիայով նկարը
•	S(x)-ը մուտքին տրված x նկարի թաքնագրված ինֆորմացիա պարունակելու հավանականությունն է
     Այս հավասարման մեջ օգտագործվել է D-ի և S-ի սխալանքների գծային գումարը 
\alpha պարամետրով,որը որոշում է,թե ինչքանով է կարևոր G-ի գեներացրած նկարի ռեալիզմը թաքնակայուն կրիչ լինելու համեմատ։ Այսինքն,եթե 
\alpha-ն 1 է ապա մեծ հավանականությամբ գեներացված նկարները կլինեն ռեալիստիկ սակայն չեն լինի թաքնակայուն։
           
Նկ․ 12 Գեներատիվ մրցակցող ցանցի մոդելներն ու նրանց կապերը
     2017 թվականին կատարված հետազոտության ժամանակ օգտագործվել է տարբեր տարիքի,սեռի,մաշկի գույնի և այլ տարբեր հատկանիշներ ունեցող,հայտնի մարդկանց նկարներ։ Դրա հետևանքով,
\alpha
\le0.7-ի դեպքում գեներատորի գեներացված նկարները այնքան ոչ-ռեալիստիկ էին ստացվել,որ ընդհանրապես պիտանի չէին։ Հետևաբար աշխատության հեղինակները վերցրել էին 
\alpha
>0.7,ինչի արդյունքում բնականաբար գեներացված նկարներն ունեցել են ցածր թաքնակայունություն։ Տվյալ աշխատությունում նախքան ուսուցումը,մարդկանց նկարները ֆիլտրվելու են,թողնելով միայն սպիտակամորթ տղամարդու նկարներ։ Ինչն էլ իր հերթին կբարձրացնի G-ի գեներացված նկարների որակը։ Միևնույն ժամանակ հնարավոր կլինի 
\alpha-ին տալ այնպիսի արժեք՝ \alpha
=0.5,որը զգալիորեն կբարձրացնի նկարների թաքնակայունությունը։
1.5	Մոդելների մանրամասն նկարագրություն
1.5.1	Տարբերակիչ
     Տարբերակիչ մոդելն իրենից ներկայացնում է երկուական դասակարգիչ։ Նրա մուտքին տրվում է 128x128 չափի նկար,այսինքն ցանցի առաջին շերտը բաղկացած է 16384 նեյրոնից։ Ելքը բաղկացած է 1 նեյրոնից,որի արժեքը ցույց է տալիս,թե որքան է հավանականությունն այն բանի,որ մուտքին տրված նկարն իրական է։ Տարբերակիչի ցանցը բաղկացած է հետևյալ շերտերից․
1.	Փաթույթային` 64 խորության,LeakyReLU ակտիվացիայով
2.	Dropout` 40%
3.	MaxPooling
4.	Dropout` 40%
5.	Փաթույթային՝ 256 խորության,LeakyReLU ակտիվացիայով
6.	Dropout` 40%
7.	MaxPooling
8.	Dropout` 40%,ելքը՝ 8192 նեյրոն
9.	Սովորական շերտ՝ 4096 նեյրոն,ելքի սիգմոիդ ակտիվացիայով
10.	Սովորական շերտ՝ 1 նեյրոն,ելքի սիգմոիդ ակտիվացիայով
1.5.2	Թաքնավերլուծիչ
     Տարբերակիչ մոդելը նույնպես երկուական դասակարգիչ է։ Նրա մուտքին տրվում է 128x128 չափի նկար իսկ ելքը բաղկացած է 1 նեյրոնից,որի արժեքը ցույց է տալիս,թե որքան է հավանականությունն այն բանի,որ մուտքին տրված նկարում որևէ թաքնագրված ինֆորմացիա կա։ Տարբերակիչի ցանցը բաղկացած է նույն շերտերից ինչ տարբերակիչը։
1.5.3	Գեներատոր
     Գեներատորը մուտքին ստանում է 100 երկարության աղմուկ,իսկ ելքում տալիս է 128x128 չափի մի նկար։ Այդ նկարը հանդիսանում է թաքնագրության կոնտեյներ։ Գեներատորի մոդելը բաղկացած է հետևյալ շարտերից․
1.	100 նեյրոն
2.	12544 նեյրոն
3.	BatchNormalization՝ տանգենցյալ ակտիվացիայով
4.	Reshape(7x7x256)
5.	Dropout՝ 40%
6.	UpSampling
7.	DeConvolution
8.	BatchNormalization` ReLU ակտիվացիայով
9.	UpSampling
10.	DeConvolution
11.	BatchNormalization` ReLU ակտիվացիայով
12.	DeConvolution
13.	BatchNormalization` ReLU ակտիվացիայով
14.	DeConvolution
1.5.4	Մրցակցող մոդելներ
     2 մրցակցող մոդելների զույգերն են՝ գեներատոր-տարբերակիչ և գեներատոր-թաքնավերլուծիչ այս 2 մոդելների ուսուցման ժամանակ մենք չենք թարմացնում տարբերակիչի և թաքնավերլուծիչի կշիռները,քանի որ այս դեպքում մեր խնդիրը գեներատորի որակը լավացնելն է։ Տարբերակիչն ու թաքնավերլուծիչը թարմացնում են իրենց կշիռները ուսուցման այլ տակտերի ընթացքում։ 
     Ուսուցումը բաղկացած է հետևյալ տակտերից՝
1.	Դիսկրիմինատորի ուսուցում
2.	Թաքնավերլուծիչի ուսուցում
3.	Գեներատոր-Դիսկրիմինատոր մրցակցող մոդելների ուսուցում
4.	Գեներատրո-Թաքնավերլուծիչ մրցակցող մոդելների ուսուցում
     Ընդ որում առաջին երկու տակտերի ընթացքում թարմացվում են դիսկրիմինատրոի և թաքնավերլուծիչի կշիռները,իսկ վերջի երկու ուսուցումներն ուղղված են գեներատորի որակի լավացմանը,այսինքն միայն գեներատորի կշիռներն են թարմացվում։ 
     Առաջին տակտի ընթացքում տարբերակիչի մուտքին տրվում են ինչպես գեներատորի գեներացրած նկարները,այնպես էլ իրական նկարներ։ Երկրորդ տակտի ընթացքում թաքնավերլուծիչին տրվում են գեներատորի գեներացված նկարները՝ որպես առանց ինֆորմացիայի թաքնագրման և թաքնագրումով։ Ընդ որում թաքնագրման համար օգտագործվում է պարզագույն «LSB-թաքնագրում» ալգորիթմը։
1.6	Ուսուցման տվյալներ
     Ուսուցման ընթացքում օգտագործվում է հայտնի աստղերի նկարներ։ Նկարները վերցված են հանրահայտ «Kaggle» կայքից։ Կայքում տրամադրված նկարների քանակը գերազանցում է 2 միլիոնը։ Ընդ որում բոլոր նկարները ունեն մոտ 40 հատկություններ որոնք հեշտացնում են նկարների ֆիլտրացիան։ Ինչպես կամայական մոդելի ուսուցման դեպքում մեր դեպքում նույնպես շատ կարևոր է մուտքային տվյալների ֆիլտրացիան։ Հենց այդ պատճառով էլ մոդելների ուսուցմանն անցնելուց առաջ նախ կատարվում է նկարների ֆիլտրացիա։ Հիմնվելով նկարների հատկությունների վրա ընտրվում են միայն այն նկարները,որոնք՝ 
•	վնասված չեն
•	պատկերված է տղամարդ
•	չունեն բեղ
•	չեն կրում վզնոց
Այս կրիտերիաները թույլ են տալիս ավելի ռեալիստիկ նկարների գեներատոր ուսուցանել։ 
Գլուխ 3.	Բնապահպանություն
1.1	Էկոլոգիական փորձաքննության նպատակները և խնդիրները
     
     Բնապահպանական կազմակերպությունների,կոմիտեների և հասարակական կազմակերպությունների գործունեության հիմնական ուղղություններից մեկը էկոլոգիական փորձաքննությունն է:
     Համաձայն Հայաստանի Հանրապետության  «Մթնոլորտային օդի պահպանության մասին» «Շրջակա միջավայրի վիա ազդեցության փորձաքննության մասին» օրենքների՝ շրջակա միջավայրի վրա ազդեցության (էկոլոգիական) փորձաքննությունը պետության կողմից անցկացվող պարտադիր գործունեություն է,որի հիմնական նպատակն է կանխորոշել,կանխարգելել կամ նվազագույնի հասցնել հայեցակարգի ն.նախատեսվող գործունեության վնասակար ազդեցությունը մարդու առողջության,շրջակա միջավայրի,տնտեսական և սոցիալական բնական զարգացման վրա:
     Շրջակա միջավայրի վրա ազդեցության փորձաքննությունը ելնում է՝
-	մարդու առողջության,բնականոն ապրելու և ստեղծագործելու համար բարենպաստ շրջակա միջավայր ունենալու իրավունքից,
-	բնական պաշարների արդյունավետ,համալիր և բանական օգտագործման պահանջներից,
-	էկոլոգիական համակարգերի հավասարակշռության և բնության մեջ գոյություն ունեցող բույսերի ն կենդանիների բոլոր տեսակների պահպանման անհրաժեշտությունից` նկատի ունենալով ներկա և ապագա սերունդների շահերը:
-	էկոլոգիական փորձաքննությունը հատուկ ստեղծված մարմինների,խմբերի առանձին փորձագետների փորձաքննական գործունեության տեսակ է` հիմնված փորձաքննման օբյեկտի միջառարկայական՝ էկոլոգա-տնտեսական-սոցիալական հետազոտման,ստուգման և գնահատման վրա,նպատակ ունենալով դրա իրականացման մասին որոշման կայացումը այն անձի կողմից,ով իրավասու է կայացնելու այդպիսի որոշում: Պետք է նկատի ունենալ,որ Հայաստանի Հանրապետությունում իրականացվում է ինչպես պետական,այնպես էլ հասարակական էկոգիական փորձաքննություն:
     Պետական էկոլոգիական փորձաքննությունը կանխարգելող հսկողության կազմակերպչական իրավական ձն է: Միաժամանակ,այն դուրս է գալիս «հսկողություն» հասկացության սահմաններից` հանդիսանալով կառավարչական գործունեության ինքնուրույն տեսակ: Պետական էկոլոգիական  փորձաքննությունը պետական մարմինների և փորձաքննության հանձնախմբի հատուկ համալիր գործունեություն է: Պետական էկոլոգիական փորձաքննության նպատակը շրջակա բնական միջավայրի պաշտպանության և էկոլոգիական անվտանգության պահանջներին փորձաքննության օբյեկտների համապտասպանությունը ստուգելը և գնահատելն է։ 
     Պետական էկոլոգիական փորձաքննության սկզբունքներն ամրագրված են օրենսդրորեն և նախատեսում են առաջին հերթին` փորձաքննության պարտադիր անցկացումը: Պետական էկոլոգիական փորձաքննությունը պետք է նախորդի տնտեսական որոշման կայացմանը՝ նպատակ ունենալով կանխարգելելու շրջակա միջավայրի վրա հնարավոր վնասակար ազդեցությունը: էկոլոգիական փորձաքննության անցկացումը պարտադիր է բոլոր նախագծերի ն ծրագրերի համար: Որպես պարտադիր պետական էկոլոգիական փորձաքննության երաշխավոր նախատեսվում է այն հանգամանքը,որ նախագծերի և ծրագրերի աշխատանքների ֆինանսավորումը հնարավոր է միայն փորձաքննության դրական եզրակացության առկայության դեպքում: էկոլոգիական փորձաքննությունը հանդես է գալիս որպես շրջակա բնական միջավայրի պահպանության մեխանիզմի գործելու երաշխավոր:
     Պետական էկոլոգիական փորձաքննության եզրակացությունների գիտական հիմնավորվածության և օրինականության սկզբունքն արտացոլում է դրա երկու ուղղությունները - գիտական և վարչաիրավական: 
     Փորձաքննությունը գիտահետազոտական գործընթաց է,հետնաբար,այն պետք է իրականացվի ժամանակակից գիտա-տեխնիկական մակարդակով,գիտական հետազոտությունների նոր ձների ե մեթոդների օգտագործմամբ,որակյալ գիտնական-փորձագետների ընդգրկմամբ։ Աշխատանքի արդյունքը պետք է լինի ոչ միայն թույլ տրված էկոլոգիական նորմատիվների խախտումների արձանագրումը,այլ նաև դրանց հետնանքների գիտականորեն հիմնավորված գնահատումը թերությունների ուղղման և վերացման համար,որոշում կայացնող մարմիններին երաշխավորությունների տրամադրումը` ինչպես նան փորձաքննվող նախագծերի և օբյեկտների ամենաարդյունավետ ձնով իրականացման պայմանների կանխատեսումը:
     Պետական էկոլոգիական փորձաքննության անկախության,արտագերատեսչակամության սկզբունքը նշանակում է,որ դրա արդյունավետության պարտադիր պայմանը փորձաքննություն կազմակերպող և իրականացնող մարմինների ֆինանսական անկախությունն է,փորձագետների արտահաստիքային կարգավիճակը:
     Կազմակերպորեն պետական էկոլոգիական փորձաքննությունը այնպիսի համակարգ է,որի կառուցվածքն ուղղված է պետական էկոլոգիական փորձաքննության արտագերատեսչականության ապահովմանը: Փորձաքննության հանձնախձբերի,խմբերի ղեկավարությունը,ինչպես նան փորձաքննության անցկացումը իրականացվում են հիմնականում արտահաստիքային փորձագետների կողմից:
     Փորձաքննության ֆինանսական անկախությունն ապահովվում է նրանով,որ այն ֆինանսավորվում է Հայաստանի Հանրապետության բյուջեից ն այն միջոցների հաշվին,որոնք ստացվում են պատվիրատուներից փորձաքննության անցկացման,այդ թվում` փորձաքննության կրկնակի անցկացման համար: Պատվիրատուների թվարկած ֆինանսական միջոցները ծախսվում են բացառապես պետական էկոլոգիական փորձաքննության վրա՝ դրա անցկացման համար կազմված նախահաշվին լիովին համապատասխան: էկոլոգիական փորձաքննության ոլորտում հատուկ լիագորված պետական մարմինը պատասխանատվություն է կրում այդ միջոցների նպատակային օգտագործման համար:
     Օբյեկտի փորձաքննության իրականացման դեպքում դրա անցկացման ընթացքի,ընդունված որոշումների ե կառավարման մարմինների կողմից դրանք հաշվի առնելու վերաբերյալ տեղեկատվությունը պետք է հասանելի լինի բնակչության լայն զանգվածների համար: Կազմակերպորեն փորձաքննության վերաբերյալ աշխատանքը պետք է կառուցված լինի այնպես,որ հասարակական կազմակերպությունները և քաղաքացիները տեղեկություն ստանան և կարողանան որոշում կայացնող մարմիններին ի գիտություն հասցնել իրենց դիրքորոշումը:
          Թվարկած օբյեկտները ենթակա են պետական էկոլոգիական փորձաքննության` անկախ դրանց նախահաշվային արժեքից ե պատկանելությունից: Այս ճանապարհով վերացվում են գերատեսչական խոչընդոտները,այսինքն` պետական փորձաքննության ենթակա են ինչպես քաղաքացիական,այնպես էլ ռազմական պաշտպանական օբյեկտները: 
     Էկոլոգիական փորձաքննության օբյեկտներին են վերաբերում բնօգտագործման համար տրված լիցենզիաների էկոլոգիական,ինչպես նան սերտիֆիկատների էկոլոգիական հիմնավորումները:
     Պետական էկոլոգիական փորձաքննության եզրակացությունը փորձաքննող հանձնաժողովի կողմից պատրաստված փաստաթուղթ է,որը բովանդակում է փորձաքննված գործունեության թույլատրելիության ե պետական էկոլոգիական փորձաքննության օբյեկտի հնարավոր իրականացման վերաբերյալ հիմնավորված եզրակացություններ: Այդ փաստաթուղթը պետք է հավանության արժանանա փորձաքննական հանձնաժողովի ցուցակային կազմի որակյալ մեծամասնության կողմից:
     Պատրաստված փաստաթուղթը պետական էկոլոգիական փորձաքննության եզրակացության կարգավիճակ ձեռք է բերում էկոլոգիական փորձաքննության ոլորտում հատուկ երաշխավորված պետական մարմնի կողմից հաստատ հետո: Պետական էկոլոգիական փորձաքննության դրական եզրակացությունը իրավաբանական ուժ ունի այն ժամանակահատվածում,որ որոշել է էկոլոգիական փորձաքննության ոլորտում պետական հատուկ երաշխավորված մարմինև,ը պետական էկոլոգիական փորձաքննության օբյեկտի ֆինանսավորման իրականացման պարտադիր պայմաններից մեկն է:
     Պետական էկոլոգիական փորձաքննության բացասական եզրակացության իրավական հետնանքը պետական էկոլոգիական փորձաքննության օբյեկտի իրականացման արգելումն է: Բացասական եզրակացության դեպքում պատվիիատուին իրավունք է տրվում կրկին անգամ ներկայացնելու նյութերը պետական էկոլոգիական փորձաքննության։ Այս դեպքում պարտադիր պայման է բացասական եզրակացությունում նշված դիտողությունների վերացումը: Բացի դրանից,պատվիրատուն իրավունք ունի եզրակացությունը վիճարկելու դատական կարգով: 
     Հասարակական էկոլոգիական փորձաքննությունը կազմակերպվում և անց է կացվում քաղաքացիների,հասարակական կազմակերպությունների (միավորումների),ինչպես նան տեղական ինքնակառավարման մարմինների նախաձեռնությամբ: Այդպիսի փորձաքննություն անցկացնում են գիտական կոլեկտիվները,հասարակական միավորումները: Գործնականում խոսքը առավելապես ժամանակավոր կոլեկտիվների,հանձնախմբերի ն խմբերի մասին է։ Հասարակական միավորումներ ասելով պետք է հասկանալ քաղաքացիների կամավոր միավորումները։
     Հասարակական կազմակերպություններն իրավունք ունեն.
-	պատվիրատուից ստանալու էկոլոգիական փորձաքննության ենթակա փաս- տաթղթերը,
-	ծանոթանալու նորմատիվտեխնիկական փաստաքղթերին,որոնցով սահմանվում են պետական էկոլոգիական փորձաքննության անցկացման պահանջները,
-	իրենց ներկայացուցիչների միջոցով,որպես դիտորդ,մասնակցելու պետական էկոլոգիական փորձաքննության փորձաքննական հանձնաժողովի նիստերին ն դրանցում հասարակական էկոլոգիական փորձաքննության եզրակացության քննարկմանը:
     Հասարակական էկոլոգիական փորձաքննություն կազմակերպող հասարակական կազմակերպությունները պարտավոր են տեղեկացնել բնակչությանը դրա սկզբի և արդյունքների վերաբերյալ: 
     Հասարակական էկոլոգիական փորձաքննության եզրակացությունը կրում է երաշխավորական,տեղեկատվական բնույթ: Սակայն այն դառնում է իրավաբանորեն պարտադիր դրա արդյունքների պետական էկոլոգիական փորձաքննության համապապասխան մարմինների կողմից հաստատվելուց հետո: 
     Հասարակական փորձաքննական կոլեկտիվների անդամները իրենց փորձաքննական գնահատականների ճշտության,հիմճնավորվածության համար պատասխանատվություն են կրում` համաձայն Հայաստանի Հանրապետության օրենսդրության:
     Չնայած հասարակական և պետական էկոլոգիական փորձաքննության նպատակները համընկնում են,սակայն դրանց խնդիրները տարբեր են: Որպես կանոն,հասարակական փորձաքննությունը անմիջական փորձաքննության խնդիրների հետ միասին նպատակ ունի պետական մարմինների ուշադրությունը սնեռելու կոնկրետ օբյեկտին,էկոլոգիական վտանգավորության վերաբերյալ գիտականորեն հիմնավորված տեղեկատվությունը հասու դարձնելու լայն հասարակայնությանը: 
Գլուխ 4.	Կենսագործունեության անվտանգություն
1.1	Կլաստերներից առաջացած աղմուկի ազդեցությունը մարդու վրա
          Ձայն հասկացությունը,որպես կանոն,ասոցացվում է մարդու լսողական զգացողությունների հետ,ով նորմալ լսողություն ունի: Լսողական զգացողությունները առաջ են գալիս առաձգական միջավայրի տատանումներից,որոնք իրենցից ներկայացնում են գազ,հեղուկ կամ պինդ միջավայրում տարածվող և մարդու լսողական ապարատի վրա ազդող մեխանիկական տատանումներ: Ընդ որում,միջավայրի այդ տատանումները որպես ձայն ընկալվում են միայն հաճախականությունների որոշակի միջակայքում: 
     Մարդն ընդունակ է որպես ձայն ընկալելու օդի 16-20000 Հց հաճախականությամբ տատանումները: 16 Հց-ից փոքր հաճախականությամբ տատանումները անվանում են ինֆրաձայն և ընկալվում են միայն որպես թրթռոցներ,իսկ 20000 Հց-ից բարձր հաճախականությամբ տատանումները անվանում են ուլտրաձայն և մարդու կողմից լսողությամբ չեն ընկալվում
∶ 
     Լսելիության միջակայքից ցածր և բարձր տատանման հաճախականությունները կոչվում են,համապատասխանաբար,ինֆրաձայնային և ուլտրաձայնային,դրանք կապ չունեն մարդու լսողական զգացողությունների հետ և ընկալվում են որպես միջավայրի ֆիզիկական ազդեցություններ
∶ 
     Եթե հոծ միջավայրում գրգռվեն տատանումներ,ապա նրանք կտարածվեն բոլոր ուղղություններով: Ակնառու օրինակ են հանդիսանում ալիքների տատանումները ջրի վրա: Ընդ որում պետք է տարբերել մեխանիկական տատանումների տարածման արագությունը և գրգռող ազդեցության տարածման արագությունը
∶
     Ֆիզիկական տեսակետից տատանման տարածումը կայանում է մեկ մոլեկուլից մյուսին շարժման իմպուլսի փոխանցման մեջ: Առաձգական միջմոլեկուլային կապերի շնորհիվ յուրաքանչյուր մոլեկուլի շարժումը կրկնում է նախորդի շարժումը: Իմպուլսի փոխանցումը պահանջում է ժամանակի որոշակի ծախսում,ինչի արդյունքում դիտման կետերում մոլեկուլների շարժումը տեղի է ունենում տատանումների գրգռման գոտում մոլեկուլների շարժման համեմատ ուշացումով: Այսպիսով,տատանումները տարածվում են որոշակի արագությամբ: 
     Ձայնային ալիքի տարածման արագությունը միջավայրի ֆիզիկական հատկությունն է
∶ Կախված տատանումների գռգռման եղանակից տարբերում են ալիքների մի քանի տեսակներ․
•	հարթ,որը ստեղծվում է հարթ տատանվող մակերևույթի միջոցով
•	գլանաձև,որը ստեղծվում է գլանի շառավղային տատանվող կողային մակերևույթի միջոցով
•	գնդային,որը ստեղծվում է բաբախող գնդի տիպի տատանումների կետային աղբյուրի միջոցով
     Ձայնային ալիքը բնութագրող հիմնական պարամետրերն են հանդիսանում ձայնային ալիքի երկարությունը,ալիքի տարածման արագությունը,տատանման հաճախությունը,ձայնային ճնշումը,ձայնի ինտենսիվությունը
∶ 
     Մարդու հիմնական զգայարաններից լսողությունը շատ մեծ դեր է խաղում նրա կյանքում: Այն թույլ է տալիս մարդուն տիրապետել ձայնային ինֆորմացիոն դաշտերին:
     Շրջակա միջավայրի հագեցումը բարձր ինտենսիվությամբ աղմուկներով բերում է ձայնային ինֆորմացիայի աղավաղման և մարդու լսողական ակտիվության խախտմանը: 135-140 դԲ արժեքի ձայնային գռգռիչների դեպքում,մարդու ներքին ականջի տարրերը առաջվա նորմալ տատանումների փոխարեն սկսում են տեղափոխվել մի կողմից մյուս կողմ,իջեցնելով խեցիում ճնշման և արտաքին միջավայրից ձայնային ճմշման տարբերությունը:
     Ցանկացած պաշտպանողական համակարգ ունի իր սահմանափակումները: Այդ իսկ պատճառով ավելցուկային աղմուկները,որոնց ազդեցության ժամանակահատվածը նույնիսկ աննշան է,առաջացնում են ներքին ականջի վնասվածք,որը լավագույն դեպքում արտահայտվում է լսողության շեմի ժամանակավոր խախտմամբ: Վերականգնման ժամանակահատվածը կարող է տևել մի քանի րոպեից մինչև մի քանի օր` կախված վնասվածքի աստիճանից:
     Արտադրական բնույթի աղմուկը փոփոխվում է ըստ ինտենսիվության և ըստ հաճախության` կախված այն մեքենաների,մեխանիզմների տիպերից և քանակությունից,որոնք օգտագործվում են տեխնոլոգիական գործընթացում: 
     Նույն ինտենսիվություն ունեցող տարբեր հաճախականությամբ ձայները մարդու կողմից ընկալվում են տարբեր բարձրությամբ: Միաժամանակ տարբեր հաճախականության և ինտենսիվության ձայները կարող են ընկալվել որպես նույն բարձրության ձայներ:
     Շրջակա միջավայրի աղտոտումը աղմուկով և դրա ազդեցությունը մարդու վրա նպատակահարմար է հաշվարկել,օգտագործելով աղմուկի էներգիայի մակարդակին համարժեք մեծությունը` Eհամ։ Վերջինս կախված է E(t)-ից՝ ժամանակի ընթացքում աղմուկի էներգիայի փոփոխությունից,որտեղ  t-ն աղմուկի ազդեցության ժամանակահատվածն է։
     Համարժեք էներգիան պետք է փոքր լինի առավելագույն թույլատրելի էներգիայից,որի դեպքում ի հայտ են գալիս բացասական հետևանքներ: Ենթադրվում է,որ վնասվածքը,որը առաջացնում է փոփոխական աղմուկի E(t) ազդեցությամբ,հավասար է այն վնասվածքին,որը առաջանում է Eհամ էներգիայով հաստատուն աղմուկը: Այսպիսով,եթե աղմուկի ազդման ժամանակամիջոցը նվազում է 2-ից 3 անգամ,ապա ձայնային էներգիայի թույլատրելի մաքսիմալ մակարդակը կարելի է ավելացնել նույնքան անգամ:
     Ակուստիկ տատանումները,որոնք դուրս են գտնվում մարդու նորմալ ձայնաընկալման տիրույթից (16…20000 Հց),նույնպես կարող են բերել լսողության վատացման: Այսպես,ուլտրաձայնը (
>20000 Հց),որը լայն տարածում ունի արդյունաբերության մեջ,հանդիսանում է լսողության վնասվածքների պատճառ,չնայած որ մարդու ականջը դրան նույնիսկ չի ընկալում: Հզոր ուլտրաձայնը ազդում է գլխուղեղի և ողնուղեղի նյարդային բջիջների վրա և առաջացնում է այրոց ականջի շրջանում և սրտխառնոց:
     Ոչ պակաս վտանգավոր է ակուստիկ տատանումների ինֆրաձայնային ազդեցությունը (
<16 Հց): Բավարար ինտենսիվության դեպքում դրանք ազդում են վեստիբյուլյար ապարատի վրա` իջեցնելով հավասարակշռությունը պահելու ունակությունը,լսելու ընկալունակությունը և առաջացնելով հոգնածություն,գրգռվածություն: 
     Յուրահատուկ դեր ունեն 7 Հց հաճախությամբ ինֆրաձայնային տատանումները: Եթե դրանք համընկնում են գլխուղեղի  ռիթմի հետ,ապա նկատվում են ոչ միայն վերը թվարկված ախտանիշները,այլև կարող է առաջանալ ներքին արյունահոսություն: 6-ից 8 Հց հաճախությամբ ինֆրաձայնը կարող է առաջացնել արյան շրջանառության խանգարում:
     Բարձր ինտենսիվությամբ աղմուկը հաճախությունների լայն տիրույթում(սկսած ինֆրաձայնից վերջացրած ուլտրաձայնով) կարող է առաջացնել գլխուղեղի և սրտի աշխատանքի խանգարումներ,շնչառական համակարգի արագության և շարժողական ակտիվության փոփոխություն: Առանձին դեպքերում աղմուկները կարող են առաջացնել վահանագեղձի չափերի փոփոխություն,արյունատար անոթների սեղմում,արյան ճնշման բարձրացում,անքնություն,հոգեկան խանգարումներ և այլն:
     Աղմուկի պատճառով լսողության կորստի գնահատման համար (ISO 1999) ստանդարտների միջազգային կազմակերպությունը հաստատել է ստանդարտ: Այդ փաստաթղթում բերվում է վնասված լսողությամբ աշխատողների սպասվող հարաբերական թիվը որպես ֆունկցիա աղմուկի էքսպոզիցիայի արժեքից:
     Օրինակ աշխատողների 22%-ը հնարավոր է կկորցնեն լսողությունը,եթե նրանք 40 տարվա ընթացքում ենթարկվեն 90 Դբ մակարդակով աղմուկի ազդեցությանը (40 ժամ աշխատանքային շաբաթվա դեպքում): Գրաֆիկի կորերը կիրառելի չեն իմպուլսային կամ բարձր ինտենսիվությամբ կարճատև աղմուկների համար:
     Մարդը,որը ենթարկվում է ինտենսիվ աղմուկի ազդեցությանը,միջին հաշվով ծախսում է 10-20% ֆիզիկական և նյարդահոգեբանական ջանքեր ավելին,քան ձայնամեկուսացված պայմաններում գտնվողը: Աղմկոտ արտադրություններում աշխատողների մոտ նկատվում է ընդհանուր բնույթի հիվանդությունների 10-15% աճ:
     
Գրականություն
1.	Steganography An Art of Hiding Data,Shashikala Channalli et al /
International Journal on Computer Science and Engineering Vol.1(3),2009
2.	https://en.wikipedia.org/wiki/Steganalysis 
3.	Generative adversarial nets.Ian Goodfellow,Jean Pouget-Abadie,Mehdi Mirza,Bing Xu,David Warde-Farley,Sherjil Ozair,Aaron Courville,and Yoshua Bengio.pp.2672–2680,2014.
4.	Unsupervised representation learning with deep convolutional generative adversarial networks.Alec Radford,Luke Metz,and Soumith Chintala.arXiv preprint arXiv:1511.06434,2015.
5.	Generative adversarial networks for image steganography.Denis Volkhonskiy,Boris Borisenko and Evgeny Burnaev
6.	https://en.wikipedia.org/wiki/Minimax
7.	Mehdi Mirza and Simon Osindero.Conditional generative adversarial nets.arXiv preprint arXiv:1411.1784,2014.
8.	Generative adversarial text to image synthesis.Scott Reed,Zeynep Akata,Xinchen Yan,Lajanugen Logeswaran,Bernt Schiele,and Honglak Lee.arXiv preprint arXiv:1605.05396,2016.
	
9.	https://www.coursera.org/learn/machine-learning/home/week/1
10.	https://www.coursera.org/learn/machine-learning/home/week/2
11.	https://www.coursera.org/learn/machine-learning/home/week/4
     1
     
